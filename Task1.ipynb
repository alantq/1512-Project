{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WYMfvCNPwpm",
    "tags": []
   },
   "source": [
    "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vA8ppgB2P0aJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "builder = tfds.builder('mnist')\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 12\n",
    "NUM_CLASSES = 10  # 10 total classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2EFLQROP2R7"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ynByMG_UP4A4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 17:30:33.953917: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load train and test splits.\n",
    "def preprocess(x):\n",
    "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
    "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
    "  return image, subclass_labels\n",
    "\n",
    "\n",
    "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
    "mnist_train = mnist_train.map(preprocess)\n",
    "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
    "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "mnist_test = tfds.load('mnist', split='test').cache()\n",
    "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZwfvW5P63q"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zINgDkA7P7BP"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Build CNN teacher.\n",
    "cnn_model = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for stpe 2\n",
    "#The shape of MNIST is 28*28*1\n",
    "cnn_model.add(tf.keras.Input(shape = (28,28,1)))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(32, 3, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides = 1))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(64, 3, strides=1, activation = 'relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides = 2))\n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
    "cnn_model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
    "cnn_model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "# Build fully connected student.\n",
    "fc_model = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "# your code start from here for step 2\n",
    "fc_model.add(tf.keras.Input(shape=(28,28,1)))\n",
    "fc_model.add(tf.keras.layers.Flatten())\n",
    "fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
    "fc_model.add(tf.keras.layers.Dense(784, activation='relu'))\n",
    "fc_model.add(tf.keras.layers.Dense(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 25, 25, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 23, 23, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 11, 11, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 7744)              0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 7744)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               991360    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,011,466\n",
      "Trainable params: 1,011,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,238,730\n",
      "Trainable params: 1,238,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()\n",
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JWGucyrQGav"
   },
   "source": [
    "# Teacher loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DhzBP6ZLQJ57"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_teacher_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  subclass_logits = cnn_model(images, training=True)\n",
    "\n",
    "  # Compute cross-entropy loss for subclasses.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "  cross_entropy_loss_value = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels, subclass_logits)\n",
    "  )\n",
    "\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS8xkuH0QbOS",
    "tags": []
   },
   "source": [
    "# Student loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "lDKia4gPQMIr"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE = 4 #temperature hyperparameter\n",
    "\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "  \"\"\"Compute distillation loss.\n",
    "\n",
    "  This function computes cross entropy between softened logits and softened\n",
    "  targets. The resulting loss is scaled by the squared temperature so that\n",
    "  the gradient magnitude remains approximately constant as the temperature is\n",
    "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
    "  a neural network.\"\n",
    "\n",
    "  Args:\n",
    "    teacher_logits: A Tensor of logits provided by the teacher.\n",
    "    student_logits: A Tensor of logits provided by the student, of the same\n",
    "      shape as `teacher_logits`.\n",
    "    temperature: Temperature to use for distillation.\n",
    "\n",
    "  Returns:\n",
    "    A scalar Tensor containing the distillation loss.\n",
    "  \"\"\"\n",
    " # your code start from here for step 3\n",
    "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  student_subclass_logits = fc_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  teacher_subclass_logits = cnn_model(images, training=False)\n",
    "  distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "  #https://medium.com/analytics-vidhya/knowledge-distillation-in-a-deep-neural-network-c9dd59aff89b\n",
    "  student_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits))\n",
    "  cross_entropy_loss_value = ALPHA * student_loss + (1-ALPHA) * distillation_loss_value\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ1uyvurQ3w4"
   },
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EtoLbp8uQ4Vl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "  \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  class_logits = model(images, training=False)\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn):\n",
    "  \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "      images, and labels.\n",
    "  \"\"\"\n",
    "\n",
    "  # your code start from here for step 4\n",
    "  #as specified in the assignment, optimizer is Adam, learning rate is 0.001 for all models\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "  for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Run training.\n",
    "    print('Epoch {}: '.format(epoch), end='')\n",
    "    for images, labels in mnist_train:\n",
    "      with tf.GradientTape() as tape:\n",
    "         # your code start from here for step 4\n",
    "\n",
    "        loss_value = compute_loss_fn(images, labels)\n",
    "\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    num_correct = 0\n",
    "    num_total = builder.info.splits['test'].num_examples\n",
    "    for images, labels in mnist_test:\n",
    "      # your code start from here for step 4\n",
    "      val1, val2, val3 = compute_num_correct(model,images,labels)\n",
    "      num_correct += val1\n",
    "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
    "        num_correct / num_total * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQL1lJdaRPT1"
   },
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-AGHbyABRPz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Class_accuracy: 98.69%\n",
      "Epoch 2: Class_accuracy: 99.00%\n",
      "Epoch 3: Class_accuracy: 98.97%\n",
      "Epoch 4: Class_accuracy: 99.03%\n",
      "Epoch 5: Class_accuracy: 99.09%\n",
      "Epoch 6: Class_accuracy: 99.13%\n",
      "Epoch 7: Class_accuracy: 99.12%\n",
      "Epoch 8: Class_accuracy: 99.22%\n",
      "Epoch 9: Class_accuracy: 99.14%\n",
      "Epoch 10: Class_accuracy: 99.17%\n",
      "Epoch 11: Class_accuracy: 99.25%\n",
      "Epoch 12: Class_accuracy: 99.23%\n"
     ]
    }
   ],
   "source": [
    "# your code start from here for step 5 \n",
    "\n",
    "#train and evaluate the teacher model\n",
    "train_and_evaluate(cnn_model, compute_teacher_loss)\n",
    "\n",
    "#train and evaluate the student model\n",
    "train_and_evaluate(fc_model, compute_student_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj1N38fnRTNB"
   },
   "source": [
    "# Test accuracy vs. tempreture curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "gX4dbazrRWIz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code start from here for step 6\n",
    "Ts = [1,2,4,16,32,64]\n",
    "epoch = [i for i in range(1,13)]\n",
    "alpha = 0.5\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNrH_1emRbGA"
   },
   "source": [
    "# Train student from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjospsxIRbQ6"
   },
   "outputs": [],
   "source": [
    "# Build fully connected student.\n",
    "fc_model_no_distillation = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for step 7\n",
    "\n",
    "\n",
    "\n",
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "def compute_plain_cross_entropy_loss(images, labels):\n",
    "  \"\"\"Compute plain loss for given images and labels.\n",
    "\n",
    "  For fair comparison and convenience, this function also performs a\n",
    "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  # your code start from here for step 7\n",
    "\n",
    "  student_subclass_logits = fc_model_no_distillation(images, training=True)\n",
    "  cross_entropy_loss = \n",
    "  \n",
    "  return cross_entropy_loss\n",
    "\n",
    "\n",
    "train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq3JTpQ4RuhR"
   },
   "source": [
    "# Comparing the teacher and student model (number of of parameters and FLOPs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V8GB2yRRuxF"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjwJ5oziRvRn"
   },
   "source": [
    "# Implementing the state-of-the-art KD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q10lybAFRvZt"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dsOmtqdieIC"
   },
   "source": [
    "# (Optional) XAI method to explain models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0IMIFW8ilPO"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 13\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
